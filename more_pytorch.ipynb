{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba54046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4443077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ead8dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c2617b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f612005e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bdbc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, 'my_tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7af5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('my_tensor')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14559760",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fef4206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d60b1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([x,y], 'more_tensors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f3e0f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]), tensor([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2, y2 = torch.load('more_tensors')\n",
    "x2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfbcc1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ee077",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a799823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1]), 'y': tensor([0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee5909ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2,20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247894e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83e36347",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = torch.load('mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b946834e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 0.0707, -0.1029, -0.1303,  ...,  0.0349, -0.0993, -0.0808],\n",
       "                      [ 0.0379,  0.0616, -0.1968,  ..., -0.1461, -0.0362, -0.1181],\n",
       "                      [-0.1707, -0.1568, -0.0609,  ..., -0.1241,  0.0120,  0.0758],\n",
       "                      ...,\n",
       "                      [-0.1496, -0.0878,  0.1481,  ..., -0.0570, -0.0909, -0.1005],\n",
       "                      [ 0.1766,  0.1130,  0.1621,  ...,  0.0873, -0.0766, -0.1203],\n",
       "                      [-0.1084,  0.2204, -0.0787,  ..., -0.1758, -0.1619, -0.1801]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([-0.0246,  0.1048,  0.0375, -0.2183, -0.1703, -0.2099,  0.0855, -0.1542,\n",
       "                       0.0039,  0.0778,  0.1646, -0.1054,  0.1446, -0.1712,  0.1332, -0.1331,\n",
       "                       0.0228,  0.0330, -0.1976,  0.1493, -0.0983,  0.1519,  0.0742,  0.0672,\n",
       "                       0.1288,  0.0567, -0.1998, -0.0148,  0.0158, -0.2025,  0.1981,  0.1010,\n",
       "                       0.0598,  0.1132, -0.1723,  0.0513, -0.0621,  0.1363, -0.0983,  0.1427,\n",
       "                       0.1923,  0.0918, -0.0679, -0.1891, -0.0936,  0.0733,  0.0226, -0.2135,\n",
       "                       0.1904, -0.0686,  0.0031, -0.0508,  0.0490, -0.2180,  0.1320,  0.1551,\n",
       "                      -0.0636,  0.1645,  0.1291, -0.0066,  0.1798,  0.1889, -0.1889, -0.0200,\n",
       "                      -0.1071,  0.1975,  0.0183, -0.1711,  0.1706,  0.0174,  0.1279, -0.0359,\n",
       "                       0.0990, -0.0440,  0.0967, -0.0533, -0.1944,  0.1779, -0.1253,  0.2118,\n",
       "                      -0.0628,  0.0872, -0.1116, -0.0272, -0.0649, -0.0278, -0.1678, -0.2140,\n",
       "                      -0.1940, -0.1235, -0.1749,  0.0211,  0.0592, -0.1962, -0.1918, -0.1296,\n",
       "                       0.1361, -0.1354,  0.2125, -0.0570,  0.0763,  0.0800, -0.1245,  0.0506,\n",
       "                      -0.0630,  0.1562,  0.2133, -0.0273, -0.1955, -0.0788, -0.1165, -0.1522,\n",
       "                       0.0009,  0.1128,  0.1914,  0.1609, -0.1228,  0.1062, -0.0276, -0.2012,\n",
       "                      -0.0698, -0.0232, -0.0926, -0.1979, -0.1585, -0.0008,  0.2222, -0.1960,\n",
       "                      -0.0467,  0.1923, -0.0314, -0.1021,  0.2168, -0.1010, -0.1793,  0.1957,\n",
       "                      -0.0128,  0.1320, -0.0708, -0.0261, -0.0207,  0.1925, -0.0991,  0.0624,\n",
       "                       0.0223, -0.0571,  0.1110, -0.0292, -0.1710,  0.0625,  0.1063, -0.1466,\n",
       "                      -0.1887, -0.1115, -0.1575,  0.1307, -0.2110,  0.2202,  0.0461, -0.1172,\n",
       "                      -0.1411, -0.0979, -0.2234,  0.0535, -0.1668,  0.1274, -0.0337, -0.1775,\n",
       "                       0.0793,  0.1872, -0.1690,  0.0499, -0.0166, -0.0281, -0.1111,  0.1437,\n",
       "                      -0.1670,  0.2087,  0.1638,  0.1164,  0.0864, -0.1815, -0.1922,  0.1603,\n",
       "                       0.2105, -0.0108,  0.0492, -0.1820, -0.0150, -0.1158, -0.0856, -0.0962,\n",
       "                      -0.1284, -0.0880,  0.1312, -0.1671,  0.0903, -0.1144, -0.0353,  0.1049,\n",
       "                       0.0491, -0.0892,  0.1567, -0.0808,  0.1568, -0.0214,  0.1992,  0.1708,\n",
       "                       0.0659, -0.0456, -0.1029,  0.0475, -0.0373,  0.1083, -0.0195,  0.1873,\n",
       "                      -0.0337, -0.0743, -0.1458, -0.0718,  0.0340, -0.1406, -0.1581,  0.2190,\n",
       "                      -0.0272, -0.0169,  0.2116,  0.0078, -0.0651, -0.1789,  0.2046,  0.0745,\n",
       "                      -0.1185,  0.2172,  0.1983,  0.0963,  0.1402,  0.0237, -0.0291,  0.0432,\n",
       "                       0.1574,  0.1017,  0.1230,  0.0135,  0.2214, -0.0975,  0.0311,  0.1776,\n",
       "                      -0.2193,  0.0041,  0.1876, -0.0556,  0.1302, -0.0617,  0.1050, -0.0431])),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.0269,  0.0431, -0.0590,  ..., -0.0610, -0.0121,  0.0211],\n",
       "                      [-0.0526,  0.0108,  0.0339,  ...,  0.0109,  0.0089,  0.0023],\n",
       "                      [-0.0020,  0.0191,  0.0527,  ...,  0.0245, -0.0612, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0472,  0.0038, -0.0473,  ..., -0.0032,  0.0305, -0.0062],\n",
       "                      [-0.0162, -0.0215,  0.0581,  ...,  0.0563,  0.0369,  0.0303],\n",
       "                      [-0.0564,  0.0101, -0.0213,  ..., -0.0121, -0.0197,  0.0509]])),\n",
       "             ('output.bias',\n",
       "              tensor([ 0.0612,  0.0376, -0.0149,  0.0386,  0.0169,  0.0063, -0.0361, -0.0593,\n",
       "                      -0.0068, -0.0595]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef74547f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d394e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa44c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i = 0):\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3e347e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c726d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(3,1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45734711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20]), 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d769b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d94be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,4)\n",
    "y = torch.randn(1,3,1)\n",
    "z =x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b801f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "138fe24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2280, -0.0600, -1.4112],\n",
       "        [-1.1221,  1.3395, -0.0856]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.randn(2,3)\n",
    "y = torch.randn(1,1)\n",
    "h*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e59da68",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x20 and 3x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x20 and 3x1)"
     ]
    }
   ],
   "source": [
    "net(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1209dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = torch.randn(2,20)\n",
    "h2 = torch.randn(3,1)\n",
    "h1_expanded = h1.unsqueeze(0).expand(3,-1,-1)\n",
    "h2_expanded = h2.unsqueeze(-1).expand(-1, 20, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c415607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 20])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "339361d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3db0182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = torch.matmul(h1_expanded, h2_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79a39e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.squeeze(h2_expanded, dim=1)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca69e153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b10dc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_gpu = h3.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8038c92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_gpu_tensor = torch.ones(2,3)\n",
    "new_gpu_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "659aaa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5385],\n",
       "        [-1.5385]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_gpu_tensor = new_gpu_tensor.cuda(0)\n",
    "net(new_gpu_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a5aca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im waqar\n",
      "this is something new in class waqar\n"
     ]
    }
   ],
   "source": [
    "class waqar():\n",
    "    def __init__(self):\n",
    "        print(\"im waqar\")\n",
    "    def something_new(self):\n",
    "        print(\"this is something new in class waqar\")\n",
    "x = waqar()\n",
    "x.something_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f70b83a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is something new in class waqar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.harry at 0x7f1ee1f61b70>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class harry(waqar):\n",
    "    def __init__(self):\n",
    "        super().something_new()\n",
    "harry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b02fa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def array_diff(list_a, list_b):\n",
    "    for a in reversed(list_a):\n",
    "        for b in reversed(list_b):\n",
    "            if a == b:\n",
    "                list_a.remove(a)\n",
    "                list_b.remove(b)\n",
    "    return list_a + list_b\n",
    "\n",
    "array_diff([1,2],[1])\n",
    "                \n",
    "                \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "476beb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_a = [1,2,4,5,6,7,8]\n",
    "list_b = [2,3,4,5,6,7]\n",
    "for i in list_a:\n",
    "    for b in list_b:\n",
    "        if i == b:\n",
    "            list_a.remove(i)\n",
    "            list_b.remove(i)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0ba690a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 6, 8]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1df6e649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 6]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7cbd925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "list_a = [1,2,4,5,6,7,8]\n",
    "list_b = [2,3,4,5,6,7]\n",
    "for i in reversed(list_a):\n",
    "    for b in reversed(list_b):\n",
    "        if i == b:\n",
    "            list_a.remove(i)\n",
    "            list_b.remove(b)\n",
    "print(list_a)\n",
    "print(list_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec99e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
